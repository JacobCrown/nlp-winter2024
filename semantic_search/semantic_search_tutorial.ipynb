{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset (GitHub issues)\n",
    "\n",
    "The issues are from the ðŸ¤— Datasets repository, we'll try to find answers for questions related to this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset\n",
    "We remove pull requests and issues without comments - they bring no information for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove columns\n",
    "We keep only columns that may be beneficial for the search task, such as issue title, issue text or comments. We do not need e.g. labels assigned to issue or username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explode comments into separate rows\n",
    "We keep the source issue (its title, link and body) for the specific comment. This allows to create meaningful context for the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add comment length column & filter by length\n",
    "\n",
    "Short comments are considered useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2964/2964 [00:00<00:00, 35596.04 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2964/2964 [00:00<00:00, 280680.87 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")\n",
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate textual columns into single wall of text\n",
    "\n",
    "This operation allows to create mini-texts that may be later vectorized for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2175/2175 [00:00<00:00, 18888.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embeddings for texts\n",
    "\n",
    "The model (`sentence-transformers/multi-qa-mpnet-base-dot-v1`) is one of the Sentence Transformers models trained for semantic search. When these models' scores for the Performance Semantic Search are considered, this model achieves the highest score.\n",
    "\n",
    "[List with models and their scores](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the [CLS] token hidden state from the transformer output and treat it as the text embedding\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: embed one text\n",
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2175/2175 [00:24<00:00, 88.87 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Embed all texts\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create FAISS index\n",
    "\n",
    "FAISS index serves as a faster alternative to vanilla search in the list of embedding vectors. It is optimized for searching similar vectors to the query vector.\n",
    "\n",
    "It needs NumPy arrays, this is why we created NumPy arrays from embeddings.\n",
    "\n",
    "Because the dataset is very small AND we make few searches, we can use Flat FAISS index (generally slow, but very accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dataset[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 682.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(len(embeddings_dataset[\"embeddings\"][0]))\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\", custom_index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find similar texts - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 30.05308723449707\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 29.860294342041016\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 29.77368927001953\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 29.164005279541016\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I think it would be very cool. I'm currently working on a cluster from Compute Canada, and I have internet access only when I'm not in the nodes where I run the scripts. So I was expecting to be able to use the wmt14 dataset until I realized I needed internet connection even if I downloaded the data already. I'm going to try option 2 you mention for now though! Thanks ;)\n",
      "SCORE: 29.1273250579834\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find texts related to other questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How can I load an XLSX file?\n",
      "\n",
      "SCORE: 15.626007080078125\n",
      "TITLE: can't load \"german_legal_entity_recognition\" dataset\n",
      "COMMENT: > Please if you could tell me more about the error?  >   > 1. Please check the directory you've been\n",
      "working on  > 2. Check for any typos    Error happens during the execution of this line:  dataset =\n",
      "load_dataset(\"german_legal_entity_recognition\")    Also, when I try to open mentioned links via\n",
      "Opera I have errors \"404: Not Found\" and \"This XML file does not appear to have any style\n",
      "information associated with it. The document tree is shown below.\" respectively.\n",
      "\n",
      "SCORE: 15.515144348144531\n",
      "TITLE: can't load \"german_legal_entity_recognition\" dataset\n",
      "COMMENT: Please if you could tell me more about the error?     1. Please check the directory you've been\n",
      "working on  2. Check for any typos\n",
      "\n",
      "SCORE: 15.150965690612793\n",
      "TITLE: viewer \"fake_news_english\" error\n",
      "COMMENT: Thanks for reporting !  The viewer doesn't have all the dependencies of the datasets. We may add\n",
      "openpyxl to be able to show this dataset properly\n",
      "\n",
      "SCORE: 15.135881423950195\n",
      "TITLE: On loading a metric from datasets, I get the following error\n",
      "COMMENT: Hi ! We support only pyarrow > 0.17.1 so that we have access to the `PyExtensionType` object.  Could\n",
      "you update pyarrow and try again ?  ```  pip install --upgrade pyarrow  ```\n",
      "\n",
      "SCORE: 15.037443161010742\n",
      "TITLE: Unable to load XTREME dataset from disk\n",
      "COMMENT: Hi @lewtun, you have to provide the full path to the downloaded file for example `/home/lewtum/..`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "questions = [\"How can I load an XLSX file?\"]\n",
    "\n",
    "\n",
    "def get_similar_comments(questions):\n",
    "    question_embeddings = get_embeddings(questions).cpu().detach().numpy()\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "        \"embeddings\", question_embeddings, k=5\n",
    "    )\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "    return samples_df\n",
    "\n",
    "\n",
    "def print_similar_comments(questions):\n",
    "    for question in questions:\n",
    "        print(f\"QUESTION: {question}\")\n",
    "        print()\n",
    "        similar_comments = get_similar_comments([question])\n",
    "        for _, row in similar_comments.iterrows():\n",
    "            print(f\"SCORE: {row.scores}\")\n",
    "            print(f\"TITLE: {row.title}\")\n",
    "            wrapped = \"\\n\".join(textwrap.wrap(row.comments, 100))\n",
    "            print(f\"COMMENT: {wrapped}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "print_similar_comments(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question `How can I load an XLSX file?` has unrelated answers because Datasets does not support XLSX out of the box, yet the searcher tries to return anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How can I load a CSV file?\n",
      "\n",
      "SCORE: 22.139331817626953\n",
      "TITLE: load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\n",
      "COMMENT: This did help to load the data. But the problem now is that I get:  ArrowInvalid: CSV parse error:\n",
      "Expected 5 columns, got 187    It seems that this change the parsing so I changed the table to tab-\n",
      "separated and tried to load it directly from pyarrow  But I got a similar error, again it loaded\n",
      "fine in pandas so I am not sure what to do.\n",
      "\n",
      "SCORE: 22.139331817626953\n",
      "TITLE: load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\n",
      "COMMENT: We should expose the [`block_size` argument](https://arrow.apache.org/docs/python/generated/pyarrow.\n",
      "csv.ReadOptions.html#pyarrow.csv.ReadOptions) of Apache Arrow csv `ReadOptions` in the\n",
      "[script](https://github.com/huggingface/datasets/blob/master/datasets/csv/csv.py).      In the\n",
      "meantime you can specify yourself the `ReadOptions` config like this:  ```python  import pyarrow.csv\n",
      "as pac   # PyArrow is installed with `datasets`    read_options = pac.ReadOptions(block_size=1e9)  #\n",
      "try to find the right value for your use-case  dataset = load_dataset('csv', data_files=files,\n",
      "read_options=read_options)  ```\n",
      "\n",
      "SCORE: 22.139331817626953\n",
      "TITLE: load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\n",
      "COMMENT: I think that the issue is similar to this one:https://issues.apache.org/jira/browse/ARROW-9612  The\n",
      "problem is in arrow when the column data contains long strings.  Any ideas on how to bypass this?\n",
      "\n",
      "SCORE: 22.139331817626953\n",
      "TITLE: load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\n",
      "COMMENT: Thanks for the fast response. I have the latest version '2.0.0' (I tried to update)  I am working\n",
      "with Python 3.8.5\n",
      "\n",
      "SCORE: 22.139331817626953\n",
      "TITLE: load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\n",
      "COMMENT: Which version of pyarrow do you have ? Could you try to update pyarrow and try again ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_comments([\"How can I load a CSV file?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question `How can I load a CSV file?` has slightly related answers because CSV is widely used and many people report issues regarding its usage. However this question is still very general which does not give precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How to write README.md for dataset?\n",
      "\n",
      "SCORE: 32.54219055175781\n",
      "TITLE: Add documentaton for dataset README.md files\n",
      "COMMENT: @lhoestq hmm - ok thanks for the answer.  To be honest I am not sure if this issue can be closed\n",
      "now.  I just wanted to point out that this should either be documented or linked in the\n",
      "documentation.  If you feel like it is (will be) please just close this.\n",
      "\n",
      "SCORE: 32.13023376464844\n",
      "TITLE: Add documentaton for dataset README.md files\n",
      "COMMENT: We're still working on the validation+documentation in this.  Feel free to keep this issue open till\n",
      "we've added them\n",
      "\n",
      "SCORE: 31.412778854370117\n",
      "TITLE: Add documentaton for dataset README.md files\n",
      "COMMENT: Hi ! We are using the [datasets-tagging app](https://github.com/huggingface/datasets-tagging) to\n",
      "select the tags to add.    We are also adding the full list of tags in #2107   This covers\n",
      "multilinguality, language_creators, licenses, size_categories and task_categories.    In general if\n",
      "you want to add a tag that doesn't exist (for example for a custom license) you must make it start\n",
      "with `other-` and then a custom tag name.    edit (@theo-m) if you ever find yourself resorting to\n",
      "adding an `other-*` tag, please do ping us somewhere so we can think about adding it to the\n",
      "\"official\" list :)\n",
      "\n",
      "SCORE: 31.410518646240234\n",
      "TITLE: Add documentaton for dataset README.md files\n",
      "COMMENT: I don't think so. Feel free to take a look at the tags of other models (example\n",
      "[here](https://huggingface.co/bert-base-uncased/blob/main/README.md)). But we should definitely have\n",
      "some docs or an app to write the tags. Feel free to open an issue in the `transformers` repo or in\n",
      "the `huggingface_hub` repo so we can discuss this\n",
      "\n",
      "SCORE: 30.99169158935547\n",
      "TITLE: Add documentaton for dataset README.md files\n",
      "COMMENT: Hi ! There's the tagging app at https://huggingface.co/datasets/tagging/ that you can use.  It shows\n",
      "the list of all the tags you can use.    It is based on all the tag sets defined in this folder:\n",
      "https://github.com/huggingface/datasets/tree/master/src/datasets/utils/resources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_comments([\"How to write README.md for dataset?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question `How to write README.md for dataset?` has strongly related answers because there exists an issue that precisely answers this question (`Add documentaton for dataset README.md files`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Do we have VoxPopuli dataset in the HuggingFace Datasets library?\n",
      "\n",
      "SCORE: 24.042560577392578\n",
      "TITLE: FileNotFound remotly, can't load a dataset\n",
      "COMMENT: This dataset will be available in version-2 of the library. If you want to use this dataset now,\n",
      "install datasets from `master` branch rather.    Command to install datasets from `master` branch:\n",
      "`!pip install git+https://github.com/huggingface/datasets.git@master`\n",
      "\n",
      "SCORE: 23.409957885742188\n",
      "TITLE: wikiann dataset is missing columns \n",
      "COMMENT: Hi !  Apparently you can get the spans from the NER tags using `tags_to_spans` defined here:    http\n",
      "s://github.com/tensorflow/datasets/blob/c7096bd38e86ed240b8b2c11ecab9893715a7d55/tensorflow_datasets\n",
      "/text/wikiann/wikiann.py#L81-L126    It would be nice to include the `spans` field in this dataset\n",
      "as in TFDS. This could be a good first issue for new contributors !    The objective is to use\n",
      "`tags_to_spans` in the `_generate_examples` method [here](https://github.com/huggingface/nlp/blob/c9\n",
      "8e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikiann.py#L292-L316) to create he `spans`\n",
      "for each example.\n",
      "\n",
      "SCORE: 23.303468704223633\n",
      "TITLE: [XGLUE] Adding new dataset\n",
      "COMMENT: Okey actually not that easy to add things like `test-de` to `datasets` => this would be the first\n",
      "dataset to have this.  See: https://github.com/huggingface/datasets/pull/802\n",
      "\n",
      "SCORE: 23.294687271118164\n",
      "TITLE: wikiann dataset is missing columns \n",
      "COMMENT: Cool ! Let me give you some context:    #### Contribution guide    You can find the contribution\n",
      "guide here:    https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md    It explains\n",
      "how to set up your dev environment in a few steps.    #### Dataset loading    Each Dataset is\n",
      "defined by a Table that have many rows (one row = one example) and columns (one column = one\n",
      "feature).  To change how a dataset is constructed, you have to modify its dataset script that you\n",
      "can find here:    https://github.com/huggingface/datasets/blob/master/datasets/wikiann/wikiann.py\n",
      "It includes everything needed to load the WikiANN dataset.  You can load locally a modified version\n",
      "of `wikiann.py` with `load_dataset(\"path/to/wikiann.py\")`.    #### Define a new column    Each\n",
      "column has a name and a type. You can see how the features of WikiANN are defined here:    https://g\n",
      "ithub.com/huggingface/datasets/blob/c98e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikian\n",
      "n.py#L245-L263    Ideally we would have one additional feature \"spans\":  ```python          \"spans\":\n",
      "datasets.Sequence(datasets.Value(\"string\")),  ```    #### Compute the content of each row    To\n",
      "build the WikiANN rows, the _generate_examples method from [here](https://github.com/huggingface/nlp\n",
      "/blob/c98e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikiann.py#L292-L316) is used. This\n",
      "function `yield` one python dictionary for each example:  ```python  yield guid_index, {\"tokens\":\n",
      "tokens, \"ner_tags\": ner_tags, \"langs\": langs}  ```    The objective would be to return instead\n",
      "something like  ```python  spans = spans = get_spans(tokens, tags)  yield guid_index, {\"tokens\":\n",
      "tokens, \"ner_tags\": ner_tags, \"langs\": langs, \"spans\": spans}  ```    Let me know if you have\n",
      "questions !\n",
      "\n",
      "SCORE: 23.09713363647461\n",
      "TITLE: Readme.md is misleading about kinds of datasets?\n",
      "COMMENT: Hi ! Yes it's possible to use image data. There are already a few of them available (MNIST, CIFAR..)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_comments(\n",
    "    [\"Do we have VoxPopuli dataset in the HuggingFace Datasets library?\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question `Do we have VoxPopuli dataset in the HuggingFace Datasets library?` has no related answers, but not because there isn't one in the dataset, but because the question has too many words that are not needed. Compare this with the following (shorter!) question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Do we have VoxPopuli dataset?\n",
      "\n",
      "SCORE: 21.702285766601562\n",
      "TITLE: Add VoxPopuli\n",
      "COMMENT: I'm happy to take this on:) One question: The original unlabelled data is stored unsegmented (see\n",
      "e.g. https://github.com/facebookresearch/voxpopuli/blob/main/voxpopuli/get_unlabelled_data.py#L30),\n",
      "but segmenting the audio in the dataset would require a dependency on something like soundfile or\n",
      "torchaudio. An alternative could be to provide the segments start and end times as a Sequence and\n",
      "then it's up to the user to perform the segmentation on-the-fly if they wish?\n",
      "\n",
      "SCORE: 21.026845932006836\n",
      "TITLE: Does both 'bookcorpus' and 'wikipedia' belong to the same datasets which Google used for pretraining BERT?\n",
      "COMMENT: No they are other similar copies but they are not provided by the official Bert models authors.\n",
      "\n",
      "SCORE: 20.259286880493164\n",
      "TITLE: Add VoxPopuli\n",
      "COMMENT: Hey @jfainberg,    This sounds great! I think adding a dependency would not be a big problem,\n",
      "however automatically segmenting the data probably means that it would take a very long time to do:\n",
      "```python  dataset = load_dataset(\"voxpopuli\", \"french\")  ```    => so as a start I think your\n",
      "option 2 is the way to go!\n",
      "\n",
      "SCORE: 20.08022689819336\n",
      "TITLE: [Feature request] Add Toronto BookCorpus dataset\n",
      "COMMENT: As far as I understand, `wikitext` is refer to `WikiText-103` and `WikiText-2` that created by\n",
      "researchers in Salesforce, and mostly used in traditional language modeling.    You might want to\n",
      "say `wikipedia`, a dump from wikimedia foundation.    Also I would like to have Toronto BookCorpus\n",
      "too ! Though it involves copyright problem...\n",
      "\n",
      "SCORE: 18.79095458984375\n",
      "TITLE: [Feature request] Add Google Natural Question dataset\n",
      "COMMENT: Super appreciate your hard work !!  I'll cross my fingers and hope easily loadable wikipedia dataset\n",
      "will come soon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_comments([\"Do we have VoxPopuli dataset?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there actually is an issue that targets this specific question (`Add VoxPopuli`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
